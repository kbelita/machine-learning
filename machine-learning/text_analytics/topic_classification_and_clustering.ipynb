{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification and Clustering\n",
    "\n",
    "The two most celebrated areas that machine learning are applied is in natural language processing and machine vision &mdash; that is machine learning that is applied to text and images. This is probably because these types of algorithms exhibit the most human-like pattern recognition and are therefore the most useful for short circuiting human-type work. \n",
    "\n",
    "In this notebook we'll take a look at how to get started with Machine Learning on text, performing both supervised topic identification (classification) and unsupervised topic modeling (clustering). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "In this tutorial we are going to use a collection of RSS feeds that have been ingested using feedparser in a project called [Baleen](https://github.com/bbengfort/baleen). Baleen uses an OPML file to specifically target blog posts of certain categories and saves them accordingly. The result is a corpus that contains 11 categories with a few thousand HTML documents inside.\n",
    "\n",
    "The document structure is as follows:\n",
    "\n",
    "    |-rss_corpus\n",
    "    |---README\n",
    "    |---books\n",
    "    |---business\n",
    "    |---cinema\n",
    "    |---cooking\n",
    "    |---data_science\n",
    "    |---design\n",
    "    |---do_it_yourself\n",
    "    |---essays\n",
    "    |---gaming\n",
    "    |---sports\n",
    "    |---tech\n",
    "\n",
    "As you can see this directory structure contains a lot of information, including the predefined topics or classes for supervised machine learning, and a README for the description of the dataset. \n",
    "\n",
    "We will load our dataset into a bundle using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CORPUS_ROOT = \"data/rss_corpus/\"\n",
    "\n",
    "def load_data(root=CORPUS_ROOT):\n",
    "    \"\"\"\n",
    "    Loads the text data into memory using the bundle dataset structure.\n",
    "    Note that on larger corpora, memory safe CorpusReaders should be used.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the README and store\n",
    "    with open(os.path.join(root, 'README'), 'r') as readme:\n",
    "        DESCR = readme.read()\n",
    "    \n",
    "    # Iterate through all the categories\n",
    "    # Read the HTML into the data and store the category in target\n",
    "    data      = []\n",
    "    target    = []\n",
    "    filenames = []\n",
    "    \n",
    "    for category in os.listdir(root):\n",
    "        if category == \"README\": continue # Skip the README\n",
    "        for doc in os.listdir(os.path.join(root, category)):\n",
    "            fname = os.path.join(root, category, doc)\n",
    "            \n",
    "            # Store information about document\n",
    "            filenames.append(fname)\n",
    "            target.append(category)\n",
    "\n",
    "            # Read data and store in data list\n",
    "            with open(fname, 'r') as f:\n",
    "                data.append(f.read())\n",
    "    \n",
    "    return Bunch(\n",
    "        data=data,\n",
    "        target=target,\n",
    "        filenames=filenames,\n",
    "        target_names=frozenset(target),\n",
    "        DESCR=DESCR,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baleen RSS Export\n",
      "=================\n",
      "\n",
      "These feeds were exported on Oct 03, 2014 at 17:30\n",
      "\n",
      "There are 5506 posts in 11 categories in this corpus as follows:\n",
      "   tech: 979 posts\n",
      "   do it yourself: 294 posts\n",
      "   cooking: 279 posts\n",
      "   cinema: 679 posts\n",
      "   gaming: 475 posts\n",
      "   essays: 46 posts\n",
      "   business: 903 posts\n",
      "   design: 449 posts\n",
      "   sports: 653 posts\n",
      "   books: 253 posts\n",
      "   data science: 496 posts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print dataset.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HTMLPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Preprocesses HTML to extract the title and the paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def parse_body_(self, soup):\n",
    "        \"\"\"\n",
    "        Helper function for dealing with the HTML body\n",
    "        \"\"\"\n",
    "        \n",
    "        if soup.find('p'):\n",
    "            # Use paragraph extraction\n",
    "            return \"\\n\\n\".join([\n",
    "                    p.text.strip() \n",
    "                    for p in soup.find_all('p') \n",
    "                    if p.text.strip()\n",
    "                ])\n",
    "        \n",
    "        else:\n",
    "            # Use raw text extraction\n",
    "            return soup.find('body').text.strip()\n",
    "    \n",
    "    def parse_html_(self, text):\n",
    "        \"\"\"\n",
    "        Helper function for dealing with an HTML document\n",
    "        \"\"\"\n",
    "        soup  = BeautifulSoup(text, 'lxml')\n",
    "        title = soup.find('title').text\n",
    "        body  = self.parse_body_(soup)\n",
    "\n",
    "        # Get rid of the soup\n",
    "        soup.decompose()\n",
    "        del soup\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'body': body\n",
    "        }\n",
    "            \n",
    "    def transform(self, texts):\n",
    "        \"\"\"\n",
    "        Extracts the text from all the paragraph tags \n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.parse_html_(text)\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "    \n",
    "class ValueByKey(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts a value from a dictionary by key.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dicts):\n",
    "        \"\"\"\n",
    "        Returns a list of values by key.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            d[self.key] for d in dicts\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Construct the Feature Extraction Pipeline\n",
    "features = Pipeline([\n",
    "    \n",
    "    # Preprocess html to extract the text.\n",
    "    ('preprocess', HTMLPreprocessor()),\n",
    "        \n",
    "    # Use FeatureUnion to combine title and body features \n",
    "    ('html_union', FeatureUnion(\n",
    "    \n",
    "        # Create union of Title and Body\n",
    "        transformer_list=[\n",
    "            \n",
    "            # Pipeline for Title  Extraction\n",
    "            ('title', Pipeline([\n",
    "                ('title_extract', ValueByKey('title')),\n",
    "                ('title_tf', CountVectorizer(\n",
    "                    max_features=4000, stop_words='english'\n",
    "                )),\n",
    "                # TODO: Add advanced TF parameters for better features\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for Task Extraction\n",
    "            ('body', Pipeline([\n",
    "                ('body_extract', ValueByKey('body')),\n",
    "                ('body_tfidf', TfidfVectorizer(stop_words='english')),\n",
    "                # TODO: Add advanced TF-IDF parameters for better features\n",
    "            ])),\n",
    "                      \n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'title': 0.45,\n",
    "            'body':  0.55,\n",
    "        },\n",
    "                \n",
    "    ))\n",
    "        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5506 documents with 53852 features in 12.564 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data  = features.fit_transform(dataset.data)\n",
    "\n",
    "print (\n",
    "    \"Processed {} documents with {} features in {:0.3f} seconds\"\n",
    "    .format(data.shape[0], data.shape[1], time.time()-start)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53852"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = features.steps[1][1].transformer_list[0][1].steps[1][1].get_feature_names()\n",
    "feature_names.extend(\n",
    "    features.steps[1][1].transformer_list[1][1].steps[1][1].get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling (Clustering)\n",
    "\n",
    "In this section we will use our features extracted from the HTML documents to do some topic clustering using NMF and LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOPICS    = 10\n",
    "N_TOP_WORDS = 20\n",
    "\n",
    "def model_topics(model, data, **kwargs):\n",
    "    \"\"\"\n",
    "    Automatic topic modeling and elucidation of topic classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    clust = model(**kwargs).fit(data)\n",
    "    \n",
    "    print \"Fit {} model in {:0.3f} seconds\\n\".format(clust.__class__.__name__, time.time()-start)\n",
    "    for idx, topic in enumerate(clust.components_):\n",
    "        print \"  Topic {}:\".format(idx)\n",
    "        for tdx in topic.argsort()[:-N_TOP_WORDS - 1:-1]:\n",
    "            print \"    - {}\".format(feature_names[tdx])\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit NMF model in 1.316 seconds\n",
      "\n",
      "  Topic 0:\n",
      "    - new\n",
      "    - york\n",
      "    - trailer\n",
      "    - new\n",
      "    - watch\n",
      "    - interstellar\n",
      "    - product\n",
      "    - iphones\n",
      "    - read\n",
      "    - home\n",
      "    - oculus\n",
      "    - prototype\n",
      "    - times\n",
      "    - mobile\n",
      "    - launches\n",
      "    - ad\n",
      "    - gets\n",
      "    - online\n",
      "    - set\n",
      "    - trailer\n",
      "\n",
      "  Topic 1:\n",
      "    - data\n",
      "    - big\n",
      "    - science\n",
      "    - read\n",
      "    - data\n",
      "    - analytics\n",
      "    - scientist\n",
      "    - comments\n",
      "    - sep\n",
      "    - mining\n",
      "    - upcoming\n",
      "    - business\n",
      "    - analytics\n",
      "    - science\n",
      "    - big\n",
      "    - kdnuggets\n",
      "    - using\n",
      "    - webcasts\n",
      "    - learning\n",
      "    - free\n",
      "\n",
      "  Topic 2:\n",
      "    - apple\n",
      "    - apple\n",
      "    - ios\n",
      "    - watch\n",
      "    - report\n",
      "    - beats\n",
      "    - music\n",
      "    - says\n",
      "    - aapl\n",
      "    - update\n",
      "    - music\n",
      "    - ios\n",
      "    - beats\n",
      "    - tim\n",
      "    - cook\n",
      "    - event\n",
      "    - 16\n",
      "    - ipad\n",
      "    - october\n",
      "    - fix\n",
      "\n",
      "  Topic 3:\n",
      "    - video\n",
      "    - tv\n",
      "    - night\n",
      "    - best\n",
      "    - trailer\n",
      "    - video\n",
      "    - late\n",
      "    - watch\n",
      "    - games\n",
      "    - music\n",
      "    - football\n",
      "    - live\n",
      "    - drone\n",
      "    - lip\n",
      "    - says\n",
      "    - movies\n",
      "    - year\n",
      "    - et\n",
      "    - wednesday\n",
      "    - says\n",
      "\n",
      "  Topic 4:\n",
      "    - iphone\n",
      "    - plus\n",
      "    - iphone\n",
      "    - plus\n",
      "    - apple\n",
      "    - aapl\n",
      "    - review\n",
      "    - new\n",
      "    - phones\n",
      "    - phones\n",
      "    - best\n",
      "    - record\n",
      "    - vs\n",
      "    - sales\n",
      "    - iphones\n",
      "    - buy\n",
      "    - big\n",
      "    - camera\n",
      "    - phone\n",
      "    - watching\n",
      "\n",
      "  Topic 5:\n",
      "    - 2014\n",
      "    - september\n",
      "    - week\n",
      "    - october\n",
      "    - links\n",
      "    - short\n",
      "    - fantastic\n",
      "    - fest\n",
      "    - design\n",
      "    - bestsellers\n",
      "    - npr\n",
      "    - critical\n",
      "    - linking\n",
      "    - day\n",
      "    - tiff\n",
      "    - london\n",
      "    - festival\n",
      "    - highlights\n",
      "    - 15\n",
      "    - film\n",
      "\n",
      "  Topic 6:\n",
      "    - 3dthursday\n",
      "    - 3dprinting\n",
      "    - 3d\n",
      "    - 3d\n",
      "    - electronichalloween\n",
      "    - printing\n",
      "    - projects\n",
      "    - printed\n",
      "    - 3dprinted\n",
      "    - 3dxhalloween\n",
      "    - thursday\n",
      "    - printed\n",
      "    - halloween\n",
      "    - design\n",
      "    - printer\n",
      "    - adafruit\n",
      "    - electronics\n",
      "    - project\n",
      "    - sculptures\n",
      "    - collection\n",
      "\n",
      "  Topic 7:\n",
      "    - pi\n",
      "    - raspberrypi\n",
      "    - piday\n",
      "    - raspberry_pi\n",
      "    - raspberry\n",
      "    - pi\n",
      "    - raspberry\n",
      "    - tutorials\n",
      "    - adafruit\n",
      "    - using\n",
      "    - camera\n",
      "    - model\n",
      "    - build\n",
      "    - station\n",
      "    - piday\n",
      "    - accessories\n",
      "    - selection\n",
      "    - code\n",
      "    - kit\n",
      "    - point\n",
      "\n",
      "  Topic 8:\n",
      "    - 10\n",
      "    - know\n",
      "    - things\n",
      "    - need\n",
      "    - windows\n",
      "    - million\n",
      "    - today\n",
      "    - ways\n",
      "    - great\n",
      "    - 10\n",
      "    - make\n",
      "    - tech\n",
      "    - windows\n",
      "    - weekend\n",
      "    - opening\n",
      "    - want\n",
      "    - goog\n",
      "    - available\n",
      "    - advertising\n",
      "    - sells\n",
      "\n",
      "  Topic 9:\n",
      "    - game\n",
      "    - game\n",
      "    - vs\n",
      "    - creed\n",
      "    - assassin\n",
      "    - play\n",
      "    - thrones\n",
      "    - 15\n",
      "    - season\n",
      "    - games\n",
      "    - steam\n",
      "    - china\n",
      "    - imitation\n",
      "    - unity\n",
      "    - pass\n",
      "    - set\n",
      "    - free\n",
      "    - games\n",
      "    - 20\n",
      "    - destiny\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_topics(NMF, data, n_components=N_TOPICS, random_state=1, alpha=.1, l1_ratio=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LatentDirichletAllocation model in 9.603 seconds\n",
      "\n",
      "  Topic 0:\n",
      "    - turkey\n",
      "    - vermilionville\n",
      "    - 168located\n",
      "    - barbecued\n",
      "    - smoker\n",
      "    - andouille\n",
      "    - 1830s\n",
      "    - gumbo\n",
      "    - smoked\n",
      "    - embodies\n",
      "    - lafayette\n",
      "    - hardcover\n",
      "    - big\n",
      "    - 168prejean\n",
      "    - luxurious\n",
      "    - gumbo\n",
      "    - destiny\n",
      "    - idea\n",
      "    - andouille\n",
      "    - making\n",
      "\n",
      "  Topic 1:\n",
      "    - chevron\n",
      "    - 1washington\n",
      "    - uncommonly\n",
      "    - garlicky\n",
      "    - pastas\n",
      "    - redskins\n",
      "    - standing\n",
      "    - anchovy\n",
      "    - sauce\n",
      "    - 2014\n",
      "    - salsa\n",
      "    - september\n",
      "    - fiction\n",
      "    - week\n",
      "    - desk\n",
      "    - china\n",
      "    - stand\n",
      "    - redskins\n",
      "    - comics\n",
      "    - paperback\n",
      "\n",
      "  Topic 2:\n",
      "    - shark\n",
      "    - literary\n",
      "    - tank\n",
      "    - entire\n",
      "    - corcoran\n",
      "    - barbara\n",
      "    - gave\n",
      "    - adult\n",
      "    - killings\n",
      "    - marlon\n",
      "    - bourne\n",
      "    - banksy\n",
      "    - mhm\n",
      "    - bookends\n",
      "    - bourne\n",
      "    - marlon\n",
      "    - bemelmans\n",
      "    - lda\n",
      "    - corcoran\n",
      "    - gensim\n",
      "\n",
      "  Topic 3:\n",
      "    - giuliani\n",
      "    - rudy\n",
      "    - lawsuit\n",
      "    - hong\n",
      "    - kong\n",
      "    - duty\n",
      "    - activision\n",
      "    - noriega\n",
      "    - noriega\n",
      "    - walken\n",
      "    - manuel\n",
      "    - bitpay\n",
      "    - counsel\n",
      "    - giuliani\n",
      "    - handing\n",
      "    - cosby\n",
      "    - hook\n",
      "    - activision\n",
      "    - artsbeat\n",
      "    - dictator\n",
      "\n",
      "  Topic 4:\n",
      "    - hardcover\n",
      "    - worlds\n",
      "    - girls\n",
      "    - kernel\n",
      "    - bookends\n",
      "    - oitnb\n",
      "    - mashub\n",
      "    - pca\n",
      "    - westerfeld\n",
      "    - mishaps\n",
      "    - proofreading\n",
      "    - pasternak\n",
      "    - stonor\n",
      "    - dugard\n",
      "    - pandemonium\n",
      "    - mandel\n",
      "    - harbinger\n",
      "    - nordberg\n",
      "    - bacha\n",
      "    - freedoms\n",
      "\n",
      "  Topic 5:\n",
      "    - unknown\n",
      "    - margaret\n",
      "    - soda\n",
      "    - atwood\n",
      "    - mattress\n",
      "    - pudding\n",
      "    - atwood\n",
      "    - adolescence\n",
      "    - psychologist\n",
      "    - ovaltine\n",
      "    - puddings\n",
      "    - pichet\n",
      "    - ong\n",
      "    - pimco\n",
      "    - malty\n",
      "    - troubled\n",
      "    - therapeutic\n",
      "    - traumas\n",
      "    - dreadfulness\n",
      "    - child\n",
      "\n",
      "  Topic 6:\n",
      "    - new\n",
      "    - apple\n",
      "    - iphone\n",
      "    - data\n",
      "    - 2014\n",
      "    - read\n",
      "    - new\n",
      "    - facebook\n",
      "    - change\n",
      "    - video\n",
      "    - people\n",
      "    - used\n",
      "    - lets\n",
      "    - check\n",
      "    - uses\n",
      "    - game\n",
      "    - stolen\n",
      "    - experiments\n",
      "    - guidelines\n",
      "    - big\n",
      "\n",
      "  Topic 7:\n",
      "    - wasn\n",
      "    - chevron\n",
      "    - deleon\n",
      "    - chevron\n",
      "    - sordid\n",
      "    - ecuador\n",
      "    - year\n",
      "    - mistakes\n",
      "    - climate\n",
      "    - poker\n",
      "    - coyotes\n",
      "    - krissy\n",
      "    - case\n",
      "    - averted\n",
      "    - athena\n",
      "    - ebola\n",
      "    - just\n",
      "    - online\n",
      "    - new\n",
      "    - change\n",
      "\n",
      "  Topic 8:\n",
      "    - excellent\n",
      "    - memoir\n",
      "    - arches\n",
      "    - swansea\n",
      "    - nabokov\n",
      "    - zoumbas\n",
      "    - whereismouse\n",
      "    - _mathildes_artbook_\n",
      "    - heythererosetta\n",
      "    - isariasir\n",
      "    - lasaventurasdechaps\n",
      "    - designbeginswithl\n",
      "    - stencild\n",
      "    - bigfootstudiosart\n",
      "    - mister_vi\n",
      "    - jgbrasier\n",
      "    - shalem_bencivenga\n",
      "    - meerkatsu\n",
      "    - starshrouded\n",
      "    - chhutisultana\n",
      "\n",
      "  Topic 9:\n",
      "    - toast\n",
      "    - religious\n",
      "    - russell\n",
      "    - eimear\n",
      "    - mcbride\n",
      "    - chronicles\n",
      "    - awesommme\n",
      "    - mannn\n",
      "    - vote\n",
      "    - slathered\n",
      "    - saffron\n",
      "    - yearnings\n",
      "    - couillard\n",
      "    - separatist\n",
      "    - perilous\n",
      "    - savery\n",
      "    - paysage\n",
      "    - roeland\n",
      "    - avec\n",
      "    - animaux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_topics(LatentDirichletAllocation, data, n_topics=N_TOPICS, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Identification (Classification)\n",
    "\n",
    "In this section we will use our features extracted from the HTML documents to do some topic identification using SVM, Maximum Entropy, and Naive Bayes. (Maybe also a Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_topics(model, data, **kwargs):\n",
    "    start = time.time()\n",
    "    clf = model(**kwargs).fit(data, dataset.target)\n",
    "    \n",
    "    print \"Fit {} model in {:0.3f} seconds\\n\".format(clf.__class__.__name__, time.time()-start)\n",
    "    classification_report(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
