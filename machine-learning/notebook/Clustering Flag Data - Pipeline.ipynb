{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook duplicates the process from Clustering Flag Data.ipynb, but implements a pipeline with OneHotEncoder instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Religion from Country Flags \n",
    "\n",
    "Professor Bengfort put together a [notebook](https://github.com/georgetown-analytics/machine-learning/blob/master/examples/bbengfort/flags/flags.ipynb) using the UCI Machine Learning Repository [_flags_](https://archive.ics.uci.edu/ml/datasets/Flags) dataset to predict the religion of a country based on the attributes of their flags. \n",
    "\n",
    "What if we had the same data, without the religion column? Can we used unsupervised machine learning to draw some conclusions about the data?\n",
    "\n",
    "ðŸ‡¦ðŸ‡«ðŸ‡¦ðŸ‡½ðŸ‡¦ðŸ‡±ðŸ‡©ðŸ‡¿ðŸ‡¦ðŸ‡¸ðŸ‡¦ðŸ‡©ðŸ‡¦ðŸ‡´ðŸ‡¦ðŸ‡®ðŸ‡¦ðŸ‡¶ðŸ‡¦ðŸ‡¬ðŸ‡¦ðŸ‡·ðŸ‡¦ðŸ‡²ðŸ‡¦ðŸ‡¼ðŸ‡¦ðŸ‡ºðŸ‡¦ðŸ‡¹ðŸ‡¦ðŸ‡¿ðŸ‡§ðŸ‡¸ðŸ‡§ðŸ‡­ðŸ‡§ðŸ‡©ðŸ‡§ðŸ‡§ðŸ‡§ðŸ‡¾ðŸ‡§ðŸ‡ªðŸ‡§ðŸ‡¿ðŸ‡§ðŸ‡¯ðŸ‡§ðŸ‡²ðŸ‡§ðŸ‡¹ðŸ‡§ðŸ‡´ðŸ‡§ðŸ‡¶ðŸ‡§ðŸ‡¦ðŸ‡§ðŸ‡¼ðŸ‡§ðŸ‡·ðŸ‡®ðŸ‡´\n",
    "\n",
    "Here is some infomation about our dataset:\n",
    "\n",
    "### Data Set Information:\n",
    "\n",
    "This data file contains details of various nations and their flags. In this file the fields are separated by spaces (not commas). With this data you can try things like predicting the religion of a country from its size and the colours in its flag. \n",
    "\n",
    "10 attributes are numeric-valued. The remainder are either Boolean- or nominal-valued.\n",
    "\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "1. name:\tName of the country concerned \n",
    "2. landmass:\t1=N.America, 2=S.America, 3=Europe, 4=Africa, 4=Asia, 6=Oceania \n",
    "3. zone:\tGeographic quadrant, based on Greenwich and the Equator; 1=NE, 2=SE, 3=SW, 4=NW \n",
    "4. area:\tin thousands of square km \n",
    "5. population:\tin round millions \n",
    "6. language: 1=English, 2=Spanish, 3=French, 4=German, 5=Slavic, 6=Other Indo-European, 7=Chinese, 8=Arabic, 9=Japanese/Turkish/Finnish/Magyar, 10=Others \n",
    "7. religion: 0=Catholic, 1=Other Christian, 2=Muslim, 3=Buddhist, 4=Hindu, 5=Ethnic, 6=Marxist, 7=Others \n",
    "8. bars: Number of vertical bars in the flag \n",
    "9. stripes: Number of horizontal stripes in the flag \n",
    "10. colours: Number of different colours in the flag \n",
    "11. red: 0 if red absent, 1 if red present in the flag \n",
    "12. green: same for green \n",
    "13. blue: same for blue \n",
    "14. gold: same for gold (also yellow) \n",
    "15. white: same for white \n",
    "16. black: same for black \n",
    "17. orange: same for orange (also brown) \n",
    "18. mainhue: predominant colour in the flag (tie-breaks decided by taking the topmost hue, if that fails then the most central hue, and if that fails the leftmost hue) \n",
    "19. circles: Number of circles in the flag \n",
    "20. crosses: Number of (upright) crosses \n",
    "21. saltires: Number of diagonal crosses \n",
    "22. quarters: Number of quartered sections \n",
    "23. sunstars: Number of sun or star symbols \n",
    "24. crescent: 1 if a crescent moon symbol present, else 0 \n",
    "25. triangle: 1 if any triangles present, 0 otherwise \n",
    "26. icon: 1 if an inanimate image present (e.g., a boat), otherwise 0 \n",
    "27. animate: 1 if an animate image (e.g., an eagle, a tree, a human hand) present, 0 otherwise \n",
    "28. text: 1 if any letters or writing on the flag (e.g., a motto or slogan), 0 otherwise \n",
    "29. topleft: colour in the top-left corner (moving right to decide tie-breaks) \n",
    "30. botright: Colour in the bottom-left corner (moving left to decide tie-breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the data and set it up for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You should recognize this from the Wheat notebook\n",
    "\n",
    "URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/flags/flag.data\"\n",
    "\n",
    "def fetch_data(fname='flags.txt'):\n",
    "    \"\"\"\n",
    "    Helper method to retreive the ML Repository dataset.\n",
    "    \"\"\"\n",
    "    response = requests.get(URL)\n",
    "    outpath  = os.path.abspath(fname)\n",
    "    with open(outpath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    return outpath\n",
    "\n",
    "# Fetch the data if required\n",
    "DATA = fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data and do some simple data management \n",
    "# We are going to define the names from the features and build a dictionary to convert our categorical features.\n",
    "\n",
    "FEATS = [\n",
    "    \"name\", \"landmass\", \"zone\", \"area\", \"population\", \"language\", \"religion\", \"bars\", \n",
    "    \"stripes\", \"colours\", \"red\", \"green\", \"blue\", \"gold\", \"white\", \"black\", \"orange\", \n",
    "    \"mainhue\", \"circles\", \"crosses\", \"saltires\", \"quarters\", \"sunstars\", \"crescent\", \n",
    "    \"triangle\", \"icon\", \"animate\", \"text\", \"topleft\", \"botright\",\n",
    "]\n",
    "\n",
    "COLOR_MAP = {\"red\": 1, \"blue\": 2, \"green\": 3, \"white\": 4, \"gold\": 5, \"black\": 6, \"orange\": 7, \"brown\": 8}\n",
    "\n",
    "# Load Data \n",
    "df = pd.read_csv(DATA, header=None, names=FEATS)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we will use the dictionary to convert categoricals into int values\n",
    "\n",
    "for k,v in COLOR_MAP.items():\n",
    "    df.ix[df.mainhue == k, 'mainhue'] = v\n",
    "    \n",
    "for k,v in COLOR_MAP.items():\n",
    "    df.ix[df.topleft == k, 'topleft'] = v\n",
    "    \n",
    "for k,v in COLOR_MAP.items():\n",
    "    df.ix[df.botright == k, 'botright'] = v\n",
    "    \n",
    "df.mainhue = df.mainhue.apply(int)\n",
    "df.topleft = df.topleft.apply(int)\n",
    "df.botright = df.botright.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering is an unsupervised machine learning method. This means we don't have to have a value we are predicting. \n",
    "\n",
    "You can use clustering when you know this information as well. Scikit-learn provides a number of metrics you can employ with a \"known ground truth\" (i.e. the values you are predicting). We won't cover them here, but you can use this notebook to add some cells, create your \"y\" value, and explore the metrics described [here](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation).\n",
    "\n",
    "In the case of the flags data, we do have our \"known ground truth\". However, for the purpose of this exercise we are going to drop that information out of our data set. We will use it later with Agglomerative Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"landmass\", \"zone\", \"area\", \"population\", \"language\", \"bars\", \n",
    "    \"stripes\", \"colours\", \"red\", \"green\", \"blue\", \"gold\", \"white\", \"black\", \"orange\", \n",
    "    \"mainhue\", \"circles\", \"crosses\", \"saltires\", \"quarters\", \"sunstars\", \"crescent\", \n",
    "    \"triangle\", \"icon\", \"animate\", \"text\", \"topleft\", \"botright\",\n",
    "]\n",
    "\n",
    "X = df[feature_names]\n",
    "y = df.religion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "\n",
    "Let's look at KMeans clustering first.\n",
    "\n",
    "\"K-means is a simple unsupervised machine learning algorithm that groups a dataset into a user-specified number (k) of clusters. The algorithm is somewhat naive--it clusters the data into k clusters, even if k is not the right number of clusters to use. Therefore, when using k-means clustering, users need some way to determine whether they are using the right number of clusters.\"\n",
    "\n",
    "One way to determine the number of cluster is through the \"elbow\" method. Using this method, we try a range of values for k and evaluate the [\"variance explained as a function of the number of clusters\"](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = range(1,10)\n",
    "meandistortions = []\n",
    "\n",
    "for k in K:\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('encoder', OneHotEncoder(categorical_features=[0, 1, 4, 7, 15, 26, 27])),\n",
    "        ('estimator', KMeans(n_clusters=k, n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "    Z = clf.fit_transform(X)\n",
    "    meandistortions.append(sum(np.min(euclidean_distances(Z, clf.steps[1][1].cluster_centers_.T), axis=1)) / X.shape[0])\n",
    "\n",
    "    \n",
    "plt.plot(K, meandistortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average distortion')\n",
    "plt.title('Selecting k with the Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best. Our goal is to choose a small value of k that still has a low variance. The elbow usually represents where we start to have diminishing returns by increasing k.\n",
    "    \n",
    "However, the elbow method doesn't always work well; especially if the data is not very clustered. \n",
    "\n",
    "Based on our plot, it looks like k=3 is worth looking at. How do we measure which might be better? We can use the Silhouette Coefficient. A higher Silhouette Coefficient score relates to a model with better defined clusters. Look at the silhouette score based on some different k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline =Pipeline([\n",
    "        ('encoder', OneHotEncoder(categorical_features=[0, 1, 4, 7, 15, 26, 27])),\n",
    "        ('estimator', KMeans(n_clusters=3, n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "pipeline.fit(X)\n",
    "labels = pipeline.steps[1][1].labels_\n",
    "silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline =Pipeline([\n",
    "        ('encoder', OneHotEncoder(categorical_features=[0, 1, 4, 7, 15, 26, 27])),\n",
    "        ('estimator', KMeans(n_clusters=5, n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "pipeline.fit(X)\n",
    "labels = pipeline.steps[1][1].labels_\n",
    "silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline =Pipeline([\n",
    "        ('encoder', OneHotEncoder(categorical_features=[0, 1, 4, 7, 15, 26, 27])),\n",
    "        ('estimator', KMeans(n_clusters=4, n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "pipeline.fit(X)\n",
    "labels = pipeline.steps[1][1].labels_\n",
    "silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above, k=3 has the better score.\n",
    "\n",
    "As implemented in scikit-learn, [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) will use 8 clusters by default. Given our data, it makes sense to try this out since our data actually has 8 potential labels (look at \"religion\" in the data secription above). Based on the plot above, we should expect the silhouette score for k=8 to be less than for k=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline =Pipeline([\n",
    "        ('encoder', OneHotEncoder(categorical_features=[0, 1, 4, 7, 15, 26, 27])),\n",
    "        ('estimator', KMeans(n_clusters=8, n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "pipeline.fit(X)\n",
    "labels = pipeline.steps[1][1].labels_\n",
    "silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize what our clusters look like. The function below will plot the clusters and visulaize their silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "def silhouette_plot(X, range_n_clusters = range(2, 12, 2)):\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1\n",
    "        ax1.set_xlim([-.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        '''clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)'''\n",
    "        \n",
    "        clusterer = Pipeline([\n",
    "        ('encoder', OneHotEncoder(categorical_features=[0, 1, 4, 7, 15, 26, 27])),\n",
    "        ('estimator', KMeans(n_clusters=n_clusters, n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X.ix[:, 0], X.ix[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors)\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.steps[1][1].cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                    marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "silhouette_plot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had just used silhouette scores, we would have missed that a lot of our data is actually not clustering very well. The plots above should make us reevaluate whether clustering is the right thing to do on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the [Wikipedia page](https://en.wikipedia.org/wiki/Hierarchical_clustering) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "The [AgglomerativeClustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)  object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. \n",
    "\n",
    "The linkage criteria determines the metric used for the merge strategy:\n",
    "* Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "* Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "* Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "\n",
    "AgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html\n",
    "\n",
    "# Visualize the clustering\n",
    "def plot_clustering(X_red, X, labels, title=None):\n",
    "    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "    X_red = (X_red - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for i in range(X_red.shape[0]):\n",
    "        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),\n",
    "                 color=plt.cm.spectral(labels[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title, size=17)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Computing embedding\")\n",
    "X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "\n",
    "for linkage in ('ward', 'average', 'complete'):\n",
    "    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=8)\n",
    "    t0 = time()\n",
    "    clustering.fit(X_red)\n",
    "    print(\"%s : %.2fs\" % (linkage, time() - t0))\n",
    "\n",
    "    plot_clustering(X_red, X, clustering.labels_, \"%s linkage\" % linkage)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been compiled using several references:\n",
    "\n",
    "* [Clustering with K-Means](https://www.packtpub.com/books/content/clustering-k-means)\n",
    "* [Selecting the number of clusters with silhouette analysis on KMeans clustering](\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py)\n",
    "* [Using the elbow method to determine the optimal number of clusters for k-means clustering](https://bl.ocks.org/rpgove/0060ff3b656618e9136b)\n",
    "* [Clustering performance evaluation](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)\n",
    "* [Hierarchical clustering](http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n",
    "* [Various Agglomerative Clustering on a 2D embedding of digits](http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
